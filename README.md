# ğŸ® DualSource-RL: Reinforcement Learning for Dual Sourcing Inventory Decisions

[![Python 3.10+](https://img.shields.io/badge/python-3.10%2B-blue.svg)](https://www.python.org/)
[![License: MIT](https://img.shields.io/badge/License-MIT-green.svg)](LICENSE)
[![Open Source](https://img.shields.io/badge/Open%20Source-âœ“-brightgreen.svg)]()

A fully open-source reinforcement learning system that trains a **dual-sourcing inventory decision agent** and deploys it in an **interactive simulation game** where users can experiment with constraints and observe the model's behaviour in real time.

---

## ğŸ“‹ Table of Contents

- [Problem Description](#-problem-description)
- [Environment Dynamics](#-environment-dynamics)
- [RL Algorithm](#-rl-algorithm)
- [Project Structure](#-project-structure)
- [Quick Start](#-quick-start)
- [Training the Agent](#-training-the-agent)
- [Evaluating & Baselines](#-evaluating--baselines)
- [Launching the Game](#-launching-the-game)
- [Configuration Reference](#-configuration-reference)
- [Example Results](#-example-results)
- [License](#-license)

---

## ğŸ§© Problem Description

A **dual-source inventory system** replenishes stock from two suppliers:

| Source | Lead Time | Unit Cost | Role |
|--------|-----------|-----------|------|
| **JIT** (Just-In-Time) | 1 period (fast) | Higher | Emergency / surge |
| **LLT** (Long Lead Time) | 4-8 periods (slow) | Lower | Bulk / base |

At every time step the agent must decide **how much to order from each source** while facing:

- **Stochastic demand** (Poisson or Normal)
- **Supply fill-rate uncertainty** (random fraction of orders actually delivered)
- **Storage capacity constraints** (hard limit or soft penalty)
- **Holding cost** for excess inventory
- **Shortage penalty** for unmet demand

The objective is to **maximise total profit** (revenue minus all costs) over a planning horizon.

### Reward Function

```
reward = revenue
       âˆ’ JIT_ordering_cost
       âˆ’ LLT_ordering_cost
       âˆ’ holding_cost
       âˆ’ shortage_penalty
       âˆ’ capacity_penalty
```

---

## âš™ï¸ Environment Dynamics

### State Space

The observation vector includes:

| Component | Dimension |
|-----------|-----------|
| Current inventory level | 1 |
| JIT pipeline (in-transit) | `jit_lead_time` |
| LLT pipeline (in-transit) | `llt_lead_time` |
| Recent demand history | `demand_history_len` |
| Normalised time step | 1 |
| Remaining capacity | 1 |
| Previous actions (JIT, LLT) | 2 |

### Action Space

`Box(2)` â€” continuous values in `[0, 1]`, scaled to `[0, max_order]`:

- `action[0]` â†’ JIT order quantity
- `action[1]` â†’ LLT order quantity

### Transition Dynamics

1. Orders placed at time *t* enter the pipeline.
2. Pipeline orders from *t âˆ’ lead_time* arrive (subject to random fill rate).
3. Stochastic demand is realised.
4. Inventory is updated: `inventory += arrivals âˆ’ satisfied_demand`.
5. Capacity constraints are enforced.

### Episode Length

Configurable; default is **150 time steps**.

---

## ğŸ¤– RL Algorithm

We use **Proximal Policy Optimisation (PPO)** via [Stable-Baselines3](https://stable-baselines3.readthedocs.io/):

- **Policy**: MLP with custom feature extractor (256 â†’ 256 â†’ 128 ReLU)
- **Actor / Critic**: Separate heads (128 â†’ 64)
- **Key hyperparameters**: `lr = 3e-4`, `Î³ = 0.99`, `clip = 0.2`, `ent_coef = 0.01`

Training typically converges within **100-150k steps** (~700-1000 episodes).

---

## ğŸ“‚ Project Structure

```
DualSource-RL/
â”‚
â”œâ”€â”€ environment/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ dual_source_env.py    # Gymnasium environment
â”‚   â”œâ”€â”€ demand_model.py       # Stochastic demand generators
â”‚   â””â”€â”€ supply_model.py       # Fill-rate & disruption models
â”‚
â”œâ”€â”€ agent/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ policy_network.py     # Custom SB3 feature extractor
â”‚   â”œâ”€â”€ train.py              # PPO training script
â”‚   â””â”€â”€ evaluate.py           # Evaluation & baseline comparison
â”‚
â”œâ”€â”€ game/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ play_game.py          # Streamlit interactive dashboard
â”‚   â”œâ”€â”€ constraints.py        # Scenario parameter dataclass
â”‚   â””â”€â”€ visualization.py      # Matplotlib plotting helpers
â”‚
â”œâ”€â”€ configs/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ default_config.py     # Central configuration
â”‚
â”œâ”€â”€ models/                   # Saved trained models
â”œâ”€â”€ logs/                     # Training logs & plots
â”œâ”€â”€ notebooks/                # Jupyter notebooks (optional)
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

## ğŸš€ Quick Start

### 1. Clone & Install

```bash
git clone https://github.com/your-username/DualSource-RL.git
cd DualSource-RL
pip install -r requirements.txt
```

### 2. Train

```bash
python -m agent.train
```

### 3. Play

```bash
streamlit run game/play_game.py
```

---

## ğŸ‹ï¸ Training the Agent

```bash
# Default: 150,000 timesteps
python -m agent.train

# Custom settings
python -m agent.train --timesteps 200000 --seed 123 --lr 1e-4
```

Training produces:

| Artifact | Path |
|----------|------|
| Saved model | `models/dual_source_rl.zip` |
| Learning curve | `logs/learning_curve.png` |
| Evaluation episode | `logs/evaluation_episode.png` |

---

## ğŸ“ˆ Evaluating & Baselines

```bash
# Evaluate RL agent vs Tailored Base-Surge heuristic
python -m agent.evaluate --episodes 20
```

This compares the PPO agent against a **Tailored Base-Surge (TBS)** heuristic:

- **LLT base order**: Constant quantity each period
- **JIT surge**: Covers shortfall vs target inventory level

Output: summary table + `logs/comparison.png` bar chart.

---

## ğŸ® Launching the Game

```bash
streamlit run game/play_game.py
```

### Sidebar Controls

| Category | Parameters |
|----------|-----------|
| **Costs** | JIT cost, LLT cost, holding cost, shortage penalty |
| **Capacity** | Storage limit, LLT lead time |
| **Demand** | Distribution, mean, std deviation |
| **Disruptions** | Demand spike mode, supply disruption mode |
| **Constraint Mode** | Hard capacity limit vs soft penalty |

### Dashboard Displays

- ğŸ“¦ Inventory level & demand over time
- ğŸšš JIT vs LLT orders (stacked bar)
- ğŸ’° Cumulative profit curve
- ğŸ“Š Running service level
- Summary KPIs (profit, service level, stockouts, etc.)
- Raw episode data table

---

## âš™ï¸ Configuration Reference

All defaults live in `configs/default_config.py`:

| Parameter | Default | Description |
|-----------|---------|-------------|
| `unit_revenue` | 12.0 | Revenue per unit sold |
| `jit_unit_cost` | 8.0 | Cost per unit ordered from JIT |
| `llt_unit_cost` | 4.0 | Cost per unit ordered from LLT |
| `holding_cost_per_unit` | 1.0 | Per-unit holding cost per period |
| `shortage_penalty_per_unit` | 10.0 | Penalty per unit of unmet demand |
| `storage_capacity` | 200 | Max inventory capacity |
| `max_order` | 50 | Max order quantity per source |
| `episode_length` | 150 | Steps per episode |
| `demand_mean` | 20 | Mean demand |
| `jit_lead_time` | 1 | JIT delivery lag |
| `llt_lead_time` | 6 | LLT delivery lag |
| `total_timesteps` | 150,000 | Training steps |

---

## ğŸ“Š Example Results

After training for 150k steps, typical results:

| Metric | PPO Agent | TBS Heuristic |
|--------|-----------|---------------|
| Avg Profit | ~350-450 | ~200-300 |
| Service Level | 92-97% | 85-92% |
| Stockout Freq | 5-10% | 10-20% |
| JIT Ratio | 30-45% | 40-60% |

The RL agent learns to:
- Use LLT for steady base replenishment
- Reserve JIT for demand surge response
- Balance inventory levels to minimise holding costs
- Adapt ordering to pipeline state

---

## ğŸ“„ License

This project is released under the **MIT License**. See [LICENSE](LICENSE) for details.

---

## ğŸ™ Acknowledgements

- [Stable-Baselines3](https://stable-baselines3.readthedocs.io/) â€” PPO implementation
- [Gymnasium](https://gymnasium.farama.org/) â€” Environment API
- [Streamlit](https://streamlit.io/) â€” Interactive dashboard
- [PyTorch](https://pytorch.org/) â€” Deep learning backend

---

*Built with â¤ï¸ for the operations research & reinforcement learning community.*
